{"ast":null,"code":"var robotsParser = require(\"robots-txt-parser\");\n\nvar robots = robotsParser({\n  userAgent: \"Googlebot\",\n  // The default user agent to use when looking for allow/disallow rules, if this agent isn't listed in the active robots.txt, we use *.\n  allowOnNeutral: false // The value to use when the robots.txt rule's for allow and disallow are balanced on whether a link can be crawled.\n\n});","map":{"version":3,"sources":["/Users/raheemmcdonald/Desktop/cxcspanish/assets/js/robots-txt.config.js"],"names":["robotsParser","require","robots","userAgent","allowOnNeutral"],"mappings":"AAAA,IAAIA,YAAY,GAAGC,OAAO,CAAC,mBAAD,CAA1B;;AACA,IAAIC,MAAM,GAAGF,YAAY,CAAC;AACxBG,EAAAA,SAAS,EAAE,WADa;AACA;AACxBC,EAAAA,cAAc,EAAE,KAFQ,CAEF;;AAFE,CAAD,CAAzB","sourcesContent":["var robotsParser = require(\"robots-txt-parser\");\nvar robots = robotsParser({\n  userAgent: \"Googlebot\", // The default user agent to use when looking for allow/disallow rules, if this agent isn't listed in the active robots.txt, we use *.\n  allowOnNeutral: false // The value to use when the robots.txt rule's for allow and disallow are balanced on whether a link can be crawled.\n});\n"]},"metadata":{},"sourceType":"script"}